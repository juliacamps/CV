{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 03. Basic Motion Detection\n",
    "\n",
    "### Goals:\n",
    "* Basic motion detection \n",
    "\n",
    "Now we just want to detect whether exists movement or not in a video frame.  In motion detection, we tend to make the following assumption:\n",
    "The background of our video stream is largely static and unchanging over consecutive frames of a video. Therefore, if we can model the background, we monitor it for substantial changes. If there is a substantial change, we can detect it, this change normally corresponds to motion on our video. \n",
    "\n",
    "Obviously in the real-world this assumption can easily fail. Due to shadowing, reflections, lighting conditions, and any other possible change in the environment, our background can look quite different in various frames of a video. And if the background appears to be different, it can throw our algorithms off. That’s why the most successful background subtraction/foreground detection systems utilize fixed mounted cameras and in controlled lighting conditions.\n",
    "\n",
    "In our case, we will take an image with our cam that will contain no motion, that is, just background. Given this, we can model the background of our video stream using only this still image of the video.\n",
    "\n",
    "Let's start by importing the necessary packages and defining resize function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import datetime\n",
    "\n",
    "#Convenience resize function\n",
    "def resize(image, width=None, height=None, inter=cv2.INTER_AREA):\n",
    "    # initialize the dimensions of the image to be resized and\n",
    "    # grab the image size\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    # if both the width and height are None, then return the\n",
    "    # original image\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "\n",
    "    # check to see if the width is None\n",
    "    if width is None:\n",
    "        # calculate the ratio of the height and construct the\n",
    "        # dimensions\n",
    "        r = height / float(h)\n",
    "        dim = (int(w * r), height)\n",
    "\n",
    "    # otherwise, the height is None\n",
    "    else:\n",
    "        # calculate the ratio of the width and construct the\n",
    "        # dimensions\n",
    "        r = width / float(w)\n",
    "        dim = (width, int(h * r))\n",
    "\n",
    "    # resize the image\n",
    "    resized = cv2.resize(image, dim, interpolation=inter)\n",
    "\n",
    "    # return the resized image\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we start looping over the frames taken by our camera. However, now we are doing a big assumption: A frame taken in our cam will contain no motion and just background, therefore, we can model the background of our video stream using only this frame of the video.\n",
    "\n",
    "We’ll also define a string named text and initialize it to indicate that the room we are monitoring is “Unoccupied”. If there is indeed activity in the room, we can update this string.\n",
    "\n",
    "We’ll first resize the image down to have a width of 500 pixels — there is no need to process the large, raw images straight from the video stream. We’ll also convert the image to grayscale since color has no bearing on our motion  detection algorithm.  It’s important to understand that even consecutive frames of a video stream will not be identical! Due to tiny variations in the digital camera sensors, no two frames will be 100% the same, some pixels will most certainly have different intensity values. That said, we need to account for this and apply Gaussian smoothing to average pixel intensities. This helps smooth out high frequency noise that could throw our motion detection algorithm off.\n",
    "\n",
    "Once we press the `q` key we will store the last picture taken with the camera, which should be just background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VIDEODEV = 0\n",
    "camera = cv2.VideoCapture(VIDEODEV); assert camera.isOpened()\n",
    "time.sleep(0.25)\n",
    "firstFrame = None\n",
    "\n",
    "# loop over the frames of the video\n",
    "while True:\n",
    "    # grab the current frame and initialize the occupied/unoccupied\n",
    "    # text\n",
    "    (grabbed, frame) = camera.read()\n",
    "    text = \"Unoccupied\"\n",
    " \n",
    "    # resize the frame, convert it to grayscale, and blur it\n",
    "    frame = resize(frame, width=500)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (21, 21), 0)\n",
    "    # draw the text and timestamp on the frame\n",
    "    cv2.putText(frame, \"Room Status: {}\".format(text), (10, 20),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "    cv2.putText(frame, datetime.datetime.now().strftime(\"%A %d %B %Y %I:%M:%S%p\"),(10, frame.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0, 0, 255), 1)\n",
    " \n",
    "    # show the frame and record if the user presses a key\n",
    "    cv2.imshow(\"Security Feed\", gray)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    # if the `q` key is pressed, break from the lop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "    \n",
    "firstFrame = gray\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our background modeled via the firstFrame  variable, we can utilize it to compute the difference between the initial frame and subsequent new frames from the video stream.\n",
    "\n",
    "Computing the difference between two frames is a simple subtraction, where we take the absolute value of their corresponding pixel intensity differences:\n",
    "\n",
    "$$\\Delta = |background\\_model – current\\_frame|$$\n",
    "\n",
    "Notice that the background of the image will be clearly black. However, regions that contain motion will be much lighter. This implies that larger frame deltas indicate that motion is taking place in the image.\n",
    "\n",
    "Next, we’ll threshold the frameDelta  to reveal regions of the image that only have significant changes in pixel intensity values. If the delta is less than 25, we discard the pixel and set it to black (i.e. background). If the delta is greater than 25, we’ll set it to white (i.e. foreground).\n",
    "\n",
    "Given this thresholded image, it’s simple to apply contour detection to to find the outlines of these white regions. We start looping over each of the contours, where we’ll filter the contours with area smaller than min-area. Otherwise, if the contour area is larger than min-area, we’ll draw the bounding box surrounding the foreground and motion region. We’ll also update our text  status string to indicate that the room is “Occupied”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_area = 500\n",
    "diffDelta = 25\n",
    "camera = cv2.VideoCapture(VIDEODEV); assert camera.isOpened()\n",
    "time.sleep(0.25)\n",
    "# loop over the frames of the video\n",
    "while True:\n",
    "    # grab the current frame and initialize the occupied/unoccupied\n",
    "    # text\n",
    "    (grabbed, frame) = camera.read()\n",
    "    text = \"Unoccupied\"\n",
    "    \n",
    "    frame = resize(frame, width=500)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (21, 21), 0)\n",
    "    \n",
    "    # compute the absolute difference between the current frame and\n",
    "    # first frame\n",
    "    frameDelta = cv2.absdiff(firstFrame, gray)\n",
    "    thresh = cv2.threshold(frameDelta, diffDelta, 255, cv2.THRESH_BINARY)[1]\n",
    " \n",
    "    # dilate the thresholded image to fill in holes, then find contours\n",
    "    # on thresholded image\n",
    "    thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
    " \n",
    "    # loop over the contours\n",
    "    for c in cnts:\n",
    "        # if the contour is too small, ignore it\n",
    "        if cv2.contourArea(c) < min_area:\n",
    "            continue\n",
    "        # compute the bounding box for the contour, draw it on the frame,\n",
    "        # and update the text\n",
    "        (x, y, w, h) = cv2.boundingRect(c)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        text = \"Occupied\"\n",
    "\n",
    "    # draw the text and timestamp on the frame\n",
    "    cv2.putText(frame, \"Room Status: {}\".format(text), (10, 20),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "    cv2.putText(frame, datetime.datetime.now().strftime(\"%A %d %B %Y %I:%M:%S%p\"),(10, frame.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0, 0, 255), 1)\n",
    " \n",
    "    # show the frame and record if the user presses a key\n",
    "    cv2.imshow(\"Security Feed\", frame)\n",
    "    cv2.imshow(\"Thresh\", thresh)\n",
    "    cv2.imshow(\"Frame Delta\", frameDelta)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    " \n",
    "    # if the `q` key is pressed, break from the lop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
